{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Word2vec tutorial"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim"
   ]
  },
  {
   "source": [
    "## Dataset\n",
    "In this tutotial the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/#.YJxB6aIzbjw) dataset will be used. This dataset has full user reviews of cars and hotels. Each line represents a hotel review."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Read only the first line of the dataset and print it. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "data_file = \"reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
    "    for i, line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "source": [
    "Read and preprocess the whole dataset. Preprocessing using <b>gensim.utils.simple_preprocess()</b> function consists: tokenization, lowercasing etc. A list of tokens (words) is returned."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "documents = list(read_input(data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['oct',\n",
       " 'nice',\n",
       " 'trendy',\n",
       " 'hotel',\n",
       " 'location',\n",
       " 'not',\n",
       " 'too',\n",
       " 'bad',\n",
       " 'stayed',\n",
       " 'in']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "documents[0][:10]"
   ]
  },
  {
   "source": [
    "\n",
    "## Training\n",
    "Word2Vec uses all tokens to inernally create a vocabulary (set of unique words). After that, we need to call <b>train()</b> to start training the Word2Vec model.\n",
    "\n",
    "Under the hood we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that weâ€™re trying to learn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train or load trained model\n",
    "TRAIN = False\n",
    "MODEL_PATH = 'model/word2vec.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    model = gensim.models.Word2Vec(documents, vector_size=150, window=10, min_count=2, workers=10)\n",
    "    model.train(documents,total_examples=len(documents),epochs=10)\n",
    "    model.save(MODEL_PATH)\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load(MODEL_PATH)"
   ]
  },
  {
   "source": [
    "## Searching for similarity\n",
    "Look up top 10 words similar to particular word. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nKeyword: clean\n[('spotless', 0.7806982398033142), ('immaculate', 0.7340009212493896), ('imaculate', 0.5190461277961731), ('spacious', 0.4928922653198242), ('stylish', 0.4916374087333679), ('cleanand', 0.4911730885505676), ('roomy', 0.4907218813896179), ('compact', 0.48200443387031555), ('plush', 0.4800761938095093), ('pristine', 0.47672703862190247)]\n\nKeyword: disappointed\n[('dissapointed', 0.9344572424888611), ('disapointed', 0.8516167998313904), ('dissappointed', 0.8500403165817261), ('impressed', 0.7611351013183594), ('pleased', 0.7314178347587585), ('satisfied', 0.6967316269874573), ('diappointed', 0.6583483815193176), ('thrilled', 0.648358941078186), ('disppointed', 0.6084690690040588), ('unimpressed', 0.6064882874488831)]\n\nKeyword: poland\n[('germany', 0.6177281141281128), ('norway', 0.5802316069602966), ('spain', 0.5783727765083313), ('czech', 0.5527172684669495), ('pakistan', 0.5523957014083862), ('immigrants', 0.5462169647216797), ('ireland', 0.5447578430175781), ('edmonton', 0.5446288585662842), ('lithuania', 0.5389659404754639), ('philippines', 0.5346682667732239)]\n"
     ]
    }
   ],
   "source": [
    "words = ['clean', 'disappointed', 'poland']\n",
    "for word in words:\n",
    "    print(f'\\nKeyword: {word}')\n",
    "    print(model.wv.most_similar(positive=word, topn=10))"
   ]
  },
  {
   "source": [
    "Compute cosine similarity between two words that are present in the vocabulary using word vectors of each. The range of the score will always be between \\[-1, 1\\]."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('clean', 'clean'): 1.00\n('dirty', 'smelly'): 0.76\n('good', 'bad'): 0.52\n('wonderful', 'poland'): -0.07\n('poor', 'poland'): -0.14\n"
     ]
    }
   ],
   "source": [
    "words_pairs = [\n",
    "    ('clean', 'clean'), \n",
    "    ('dirty', 'smelly'), \n",
    "    ('good', 'bad'), \n",
    "    ('wonderful', 'poland'), \n",
    "    ('poor', 'poland'),\n",
    "]\n",
    "\n",
    "for pair in words_pairs:\n",
    "    similarity = model.wv.similarity(*pair)\n",
    "    print(f'{pair}: {similarity:.2f}')\n"
   ]
  },
  {
   "source": [
    " If you do a similarity between two identical words, the score will be 1.0, like in the case of pair (clean, clean). From the scores, it makes sense that dirty is highly similar to smelly. Pair (wonderful, poland) is decorrelated, which results from the score -0.07."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Find the odd items given a list of items."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('poland', 'norway', 'rich'): rich\n('sun', 'rain', 'bathroom'): bathroom\n('flower', 'grass', 'beer'): beer\n"
     ]
    }
   ],
   "source": [
    "words_list = [\n",
    "    ('poland', 'norway', 'rich'),\n",
    "    ('sun', 'rain', 'bathroom'),\n",
    "    ('flower', 'grass', 'beer')\n",
    "]\n",
    "\n",
    "for words in words_list:\n",
    "    odd = model.wv.doesnt_match(words)\n",
    "    print(f'{words}: {odd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
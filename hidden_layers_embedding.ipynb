{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwpypAo5FqdR"
   },
   "source": [
    "# Neural network hidden layers activation embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eryk Sikora <br>\n",
    "Krzysztof Tylka-Suleja<br>\n",
    "Aleksandra Pestka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btdHzPnUFqdf"
   },
   "source": [
    "#### In this notebook we will use fully connected neural network for a classification task to improve visualizations algorithm from previous classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bn9rP1LFqdh"
   },
   "source": [
    "In the fully connected neural network, the output of each layer is computed using the activations from the previous one. In neural network training process, each successive layer learns to extract features from data with increasingly higher levels of abstraction. In this exercise, instead of directly visualizing data, we'll try to visualize the activation of hidden layers in neural networks. Using this idea, we can improve the process of data visualization, and on the other hand, see how processing this data looks like by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIxM0b3VFqdj"
   },
   "source": [
    "In the first stage, we define simple architecture of the neural network and train it to recognize digits in the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrtM7u0cFqdk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6knEQmcFqdm"
   },
   "outputs": [],
   "source": [
    "nb_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr2ExkbjFqdn"
   },
   "source": [
    "The dropout layers have the very specific function to drop out a random set of activations in that layers by setting them to zero in the forward pass. Simple as that.\n",
    "It allows to avoid overfitting but has to be used only at training time and not at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_g1WGuYZFqdp"
   },
   "outputs": [],
   "source": [
    "# set dropout rate - fractions of neurons to drop\n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cPYWTb7Fqdr"
   },
   "outputs": [],
   "source": [
    "# build very simple neural network with 2 hidden layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTKCKu8yFqds",
    "outputId": "877e1964-f9bb-4ea1-c822-58ce24767e27"
   },
   "outputs": [],
   "source": [
    "# The binary_crossentropy loss expects a one-hot-vector as input,\n",
    "# so we apply the to_categorical function from keras.utilis to convert integer labels to one-hot-vectors.\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbcuUnt9Fqdt",
    "outputId": "1baaddab-1374-42f4-95e2-6d2c005b2a05"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCZfUIpLFqdx"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "# Put everything on grayscale\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Sc7nktkFqdy"
   },
   "outputs": [],
   "source": [
    "# split training and validation data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, train_size=5/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oqbBR5BFqdz",
    "outputId": "ff9bb5c1-da11-4577-f259-a4bacab0f631"
   },
   "outputs": [],
   "source": [
    "# show example digit\n",
    "plt.imshow(X_train[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIU4kA22Fqd0",
    "outputId": "a5333d2a-75c0-48a0-cc31-2bde0e7a0e36"
   },
   "outputs": [],
   "source": [
    "# When we have defined and compiled the model, it can be trained using the fit function.\n",
    "# We also use validation dataset to monitor validation loss and accuracy.\n",
    "\n",
    "network_history = model.fit(X_train, Y_train, batch_size=128, \n",
    "                            epochs=20, verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xg6qNHhvFqd2"
   },
   "outputs": [],
   "source": [
    "def plot_history(network_history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['accuracy'])\n",
    "    plt.plot(network_history.history['val_accuracy'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEdHxzCoFqd3",
    "outputId": "437c0f16-134c-4e41-e815-d7970733dd3e"
   },
   "outputs": [],
   "source": [
    "# fit function return keras.callbacks.History object which contains the entire history\n",
    "# of training/validation loss, accuracy and other metrices for each epoch.\n",
    "# We can therefore plot the behaviour of loss and accuracy during the training phase.\n",
    "\n",
    "plot_history(network_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmVrNDbYFqd5",
    "outputId": "5da87140-0ec1-4f5d-f83e-d0a2092593d7"
   },
   "outputs": [],
   "source": [
    "# Keras Model have summary function, that print data about model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGNzOQCCFqd6"
   },
   "outputs": [],
   "source": [
    "# We are interested in downloading the activation of hidden layers, because the dropout layers are between them,\n",
    "# we need to properly select the index of the three dense layers.\n",
    "\n",
    "get_layer_output = K.function([model.layers[0].input],\n",
    "                              [model.layers[0].output, model.layers[2].output, model.layers[4].output])\n",
    "\n",
    "layer1_output, layer2_output, layer3_output = get_layer_output([X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe6nu8NcFqd7"
   },
   "outputs": [],
   "source": [
    "train_ids = [np.arange(len(Y_train))[Y_train[:,i] == 1] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBOCji3LFqd9"
   },
   "source": [
    "The 2 graphs below are not directly related to the topic of the exercise, but they visualize very well how neuron activation actives work and for explanation are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-bO24fhFqd9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "# this animation shows what the example number 5 looks like\n",
    "# and what activations of neurons look in hidden layers of the neural network\n",
    "\n",
    "\n",
    "\n",
    "# digit to be plotted\n",
    "digit = 5\n",
    "\n",
    "# indices of frames to be plotted for this digit\n",
    "n = range(50)\n",
    "\n",
    "# initialize plots\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n",
    "\n",
    "# prepare plots\n",
    "ax1.set_title('Input Layer', fontsize=16)\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax2.set_title('Hidden Layer 1', fontsize=16)\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax3.set_title('Hidden Layer 2', fontsize=16)\n",
    "ax3.axes.get_xaxis().set_visible(False)\n",
    "ax3.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "ax4.set_title('Output Layer', fontsize=16)\n",
    "ax4.axes.get_xaxis().set_visible(False)\n",
    "ax4.axes.get_yaxis().set_visible(False)   \n",
    "\n",
    "# add numbers to the output layer plot to indicate label\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        text = ax4.text(j, i, [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, '', '']][i][j],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", fontsize=16)    \n",
    "        \n",
    "def animate(id):\n",
    "    # plot elements that are changed in the animation\n",
    "    digit_plot = ax1.imshow(X_train[train_ids[digit][id]].reshape((28,28)), animated=True)\n",
    "    layer1_plot = ax2.imshow(layer1_output[train_ids[digit][id]].reshape((16,16)), animated=True)\n",
    "    layer2_plot = ax3.imshow(layer2_output[train_ids[digit][id]].reshape((8,8)), animated=True)\n",
    "    output_plot = ax4.imshow(np.append(layer3_output[train_ids[digit][id]], \n",
    "                                       [np.nan, np.nan]).reshape((3,4)), animated=True)\n",
    "    return digit_plot, layer1_plot, layer2_plot, output_plot,\n",
    "\n",
    "# define animation\n",
    "ani = matplotlib.animation.FuncAnimation(f, animate, frames=n, interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByDvUrBlFqeB",
    "outputId": "a35e2f45-6047-47c7-ebaf-1a00ab31a07a"
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLdaXNB2FqeC"
   },
   "source": [
    "In most cases the same subset of neurons fires, while other neurons remain quiescent. This is much more obvious in the second hidden layer than in the first hidden layer and can be interpreted as the first layer pre-processesing the pixel data, while the second layer deals with pattern recognition.\n",
    "\n",
    "This effect is mainly caused by regularization forced by dropout. Dropout generally leads to the sparse weight matrices where a significant part of connection weights are close to 0. Insignificant weights are suppressed.\n",
    "\n",
    "\n",
    "Optional, nonobligatory task:\n",
    "You can easily see how the visualizations change if you comment lines responsible for the dropout \"model.add(Dropout(dropout))\".\n",
    "Remember to change \"get_layer_output\", because after removing the dropout, the dense layers will have indexes: 0,1,2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9ukRdkoFqeD"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Let's check the similarity in behavior for frames showing the same digit by looking at the ensemble properties.\n",
    "# In this case, ensemble properties refers to how the neurons behave on average\n",
    "# for a large number of frames showing the same digit.\n",
    "\n",
    "# digit to be plotted\n",
    "digit = 6\n",
    "\n",
    "# numbers of frames to be summed over\n",
    "n = np.append([1], np.linspace(5, 100, 20, dtype=int))\n",
    "\n",
    "# initialize plots\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n",
    "\n",
    "# add a counter indicating the number of frames used in the summation\n",
    "counter = ax1.text(1, 2, 'n={}'.format(0), color='white', fontsize=16, animated=True)\n",
    "\n",
    "# prepare plots\n",
    "ax1.set_title('Input Layer', fontsize=16)\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax2.set_title('Hidden Layer 1', fontsize=16)\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax3.set_title('Hidden Layer 2', fontsize=16)\n",
    "ax3.axes.get_xaxis().set_visible(False)\n",
    "ax3.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "ax4.set_title('Output Layer', fontsize=16)\n",
    "ax4.axes.get_xaxis().set_visible(False)\n",
    "ax4.axes.get_yaxis().set_visible(False)   \n",
    "\n",
    "# add numbers to the output layer plot to indicate label\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        text = ax4.text(j, i, [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, '', '']][i][j],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", fontsize=16)    \n",
    "        \n",
    "def animate(id):\n",
    "    # plot elements that are changed in the animation\n",
    "    digit_plot = ax1.imshow(np.sum(X_train[train_ids[digit][:id]], axis=0).reshape((28,28)), animated=True)\n",
    "    layer1_plot = ax2.imshow(np.sum(layer1_output[train_ids[digit][:id]], axis=0).reshape((16,16)), animated=True)\n",
    "    layer2_plot = ax3.imshow(np.sum(layer2_output[train_ids[digit][:id]], axis=0).reshape((8,8)), animated=True)\n",
    "    output_plot = ax4.imshow(np.append(np.sum(layer3_output[train_ids[digit][:id]], axis=0), \n",
    "                                       [np.nan, np.nan]).reshape((3,4)), animated=True)\n",
    "    counter.set_text('n={}'.format(id))\n",
    "    return digit_plot, layer1_plot, layer2_plot, output_plot, counter,\n",
    "\n",
    "# define animation\n",
    "ani = matplotlib.animation.FuncAnimation(f, animate, frames=n, interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSE6yZAEFqeE",
    "outputId": "85b69593-a0a8-4b4d-dac3-1444617f854c"
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNEWfaSIFqeI"
   },
   "source": [
    "After summing up the responses of as little as 20-30 frames, the pattern in the second hidden layer is almost static. After combining about 70-80 frames, also the pattern in the first hidden layer appears static. This supports the idea that only a subset of all neurons is involved in the recognition of individual digits.\n",
    "\n",
    "Especially the above plot is important when we think about use of neural networks for data visualization. We can clearly see that the activation generated by examples belonging to the same class are less chaotic than the examples themselves, therefore their visualization should give a more clustered structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRPveLDEFqeJ"
   },
   "source": [
    "# Homework\n",
    "\n",
    "- project a mnist training part into 2-dimensional space using t-SNE and UMAP.\n",
    "\n",
    "- use layer1_output and layer2_output to project first and second hidden layers of neural network into a 2-dimensional space. Also divided into a test and training set, use the same methods as the point above.\n",
    "\n",
    "- also visualize the test part.\n",
    "\n",
    "- Try to use 2-dimensional projection for classification task.\n",
    "\n",
    "- Use embeddings lerned on raw train data (and also on hidden activations of train data) to transform test data (and also hidden activations of test data) into 2-dimensional space.\n",
    "\n",
    "- Use the k-nearest neighbors algorithm to classify transformed points from the test set. Use the KNN algorithm in which you will use points from the training set as a neighbor with known class assignment. Becouse t-SNE is a non-linear, non-parametric embedding you cant use already learned t-SNE to transform new points into the existing embedded space. So for this part, use only UMAP with have fit_transform method (learn manifold) and also transform (only project new data to existing manifold). Try with few values of n_neighbors e.g [3, 5, 10]\n",
    "\n",
    "- Estimate the accuracy of classification using this approach. Use all 3 layers (raw data, 1 hidden layer, 2 hidden layer) and few values of n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtMB-r9SFqeK"
   },
   "source": [
    "## UMAP and TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metoda umożliwająca wizualizację danych przetworzonych przy użyciu UMAP i t-SNE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6W97EEbFqeK"
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def method_vis(embedding, target, title, custom_labels=None):\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    target = np.argmax(target, axis=1) # inverse of 'to_categorical' function\n",
    "    color = target.astype(int)\n",
    "    scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap=\"Spectral\", s=0.1)\n",
    "    \n",
    "    if custom_labels is not None:\n",
    "        cbar = plt.colorbar(boundaries=np.arange(11)-0.5)\n",
    "        cbar.set_ticks(np.arange(10))\n",
    "        cbar.set_ticklabels(custom_labels)\n",
    "    else:\n",
    "        # produce a legend with the unique colors from the scatter\n",
    "        legend = ax.legend(*scatter.legend_elements(),\n",
    "                        loc=\"upper right\", title=\"Classes\")\n",
    "        ax.add_artist(legend)\n",
    "    \n",
    "    plt.title(f\"{title}\", fontsize=18)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghgO4I86NiBk"
   },
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY9PliBcFqeL"
   },
   "source": [
    "**Wizualizacja częsci treningowej i testowej danych wejściowych przetworzonych przy użyciu UMAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JV_MALv3FqeM",
    "outputId": "346166f7-2180-44ce-ead8-66c9e83e659a"
   },
   "outputs": [],
   "source": [
    "reducer_umap_mnist = umap.UMAP(random_state=42)\n",
    "embedding_umap_mnist = reducer_umap_mnist.fit_transform(X_train) # learn manifold \n",
    "embedding_umap_mnist_test = reducer_umap_mnist.transform(X_test) # project new data to existing manifold\n",
    "\n",
    "method_vis(embedding_umap_mnist, Y_train, 'UMAP for MNIST training set')\n",
    "method_vis(embedding_umap_mnist_test, Y_test, 'UMAP for MNIST testing set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wizualizacja pokazuje dobrą seperację pomiędzy większością klas, jednak klasy odpowiadające cyfrom 9, 7, 4 oraz 3, 5, 8 są bardzo blisko siebie.\\\n",
    "Zbiory trenigowy, na którym trenowany był model, jak i testowy tworzą bardzo podobą wizualizację."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjC4bnt2MGxh"
   },
   "source": [
    "### UMAP for hidden **layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8BWzsYAKf1w"
   },
   "outputs": [],
   "source": [
    "# get hidden activations of test data\n",
    "test_layer1_output, test_layer2_output, test_layer3_output = get_layer_output([X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDoc2liWH13q"
   },
   "source": [
    "**Wizualizacja częsci treningowej i testowej danych wyjściowych z pierwszej warstwy sieci neuronowej przetworzonych przy użyciu UMAP**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLsP1k4zI0fg",
    "outputId": "b91c09f9-4f08-4e6d-9f03-b61aff2a5306"
   },
   "outputs": [],
   "source": [
    "layer1_umap = umap.UMAP(random_state=42)\n",
    "embedding_umap_layer1 = layer1_umap.fit_transform(layer1_output) \n",
    "embedding_umap_layer1_test = layer1_umap.transform(test_layer1_output)\n",
    "\n",
    "method_vis(embedding_umap_layer1, Y_train, 'UMAP for layer1 (training set)')\n",
    "method_vis(embedding_umap_layer1_test, Y_test, 'UMAP for layer1 (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBcn7xvkJuGU"
   },
   "source": [
    "Wizualizacja pokazuje bardzo dobrą seperację pomiędzy wszystkimi klasami. Ponownie klasy odpowiadające cyfrom 9, 7, 4 oraz 3, 5, 8 są bliżej siebie niż pozostałe, jednak są znacznie lepiej odseperowane.\\\n",
    "Ponownie zbiory trenigowy, na którym trenowany był model, jak i testowy tworzą bardzo podobą wizualizację."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wizualizacja częsci treningowej i testowej danych wyjściowych z drugiej warstwy sieci neuronowej przetworzonych przy użyciu UMAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZwX0dpAJz2x",
    "outputId": "7e7c693a-d979-4156-a69e-a3a6b89065b3"
   },
   "outputs": [],
   "source": [
    "layer2_umap = umap.UMAP(random_state=42)\n",
    "embedding_umap_layer2 = layer2_umap.fit_transform(layer2_output)\n",
    "embedding_umap_layer2_test = layer2_umap.transform(test_layer2_output)\n",
    "\n",
    "method_vis(embedding_umap_layer2, Y_train, 'UMAP for layer2 (training set)')\n",
    "method_vis(embedding_umap_layer2_test, Y_test, 'UMAP for layer2 (test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wizualizacja pokazuje najlepszą z dotychczas uzyskanych seperacji pomiędzy wszystkimi klasami. Wszystkie klasy są w niemalże jednakowych odległościach.\\\n",
    "Ponownie zbiory trenigowy, na którym trenowany był model, jak i testowy tworzą bardzo podobą wizualizację."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmLxWwguNldR"
   },
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wizualizacja częsci treningowej danych wejściowych przetworzonych przy użyciu t-SNE**\n",
    "\n",
    "TSNE is a non-linear, non-parametric embedding, so there is no possibility to use already learned t-SNE to transform new points into the existing embedded space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfZfBmJtFqeN"
   },
   "outputs": [],
   "source": [
    "reducer_tsne_mnist = TSNE(n_components=2, metric='euclidean')\n",
    "embedding_mnist_tsne = reducer_tsne_mnist.fit_transform(X_train)\n",
    "\n",
    "method_vis(embedding_mnist_tsne, Y_train, 'TSNE for MNIST training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wizualizacja pokazuje dobrą seperację pomiędzy klasami, jednak granice poszczególnych klas są bardzo rozmyte, a klasy odpowiadające cyfrom 3 i 8 przenikają się oraz widać pojedyncze elementy znajdujące się w obrębie nie swoich klas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oABBR1tlNq-W"
   },
   "source": [
    "### TSNE for hidden **layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjy5su4-PlWV"
   },
   "source": [
    "**Wizualizacja częsci treningowej i testowej danych wyjściowych z pierwszej warstwy sieci neuronowej przetworzonych przy użyciu t-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T7oGngvNqGo"
   },
   "outputs": [],
   "source": [
    "layer1_tsne = TSNE(n_components=2, metric='euclidean', perplexity=35)\n",
    "embedding_tsne_layer1 = layer1_tsne.fit_transform(layer1_output) \n",
    "\n",
    "method_vis(embedding_tsne_layer1, Y_train, 'TSNE for layer1 (training set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0I_I9wsPqqL"
   },
   "source": [
    "Wizualizacja pokazuje lepszą seperację pomiędzy klasami niż poprzedni przykład, granice poszczególnych klas są mniej rozmyte, a klasy odpowiadające cyfrom 3 i 8 nie przenikają przenikają się."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wizualizacja części treningowej i testowej danych wyjściowych z drugiej warstwy sieci neuronowej przetworzonych przy użyciu t-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibJWZUVPPsqA"
   },
   "outputs": [],
   "source": [
    "layer2_tsne = TSNE(n_components=2, metric='euclidean', perplexity=35)\n",
    "embedding_tsne_layer2 = layer2_tsne.fit_transform(layer2_output) \n",
    "\n",
    "method_vis(embedding_tsne_layer2, Y_train, 'TSNE for layer2 (training set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wizualizacja pokazuje najlepszą seperację spośród wszystkich porównywanych dla metody t-SNE. Granice poszczególnych klas bardzo ostre, znacznie mniej elementów wybiega poza granice swoich klas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def knnScore(x_train, y_train, x_test ,y_test, n_neighbors,metric=\"euclidean\"):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "    neigh.fit(x_train, y_train)\n",
    "    predicted = neigh.predict(x_test)\n",
    "    return accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kns = [3,5,10,15,20,30]\n",
    "metrics = [\"euclidean\",\"manhattan\",\"chebyshev\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "raw_accuracies = [[knnScore(embedding_umap_mnist, Y_train, embedding_umap_mnist_test, Y_test, i,metric) for metric in metrics] for i in kns]\n",
    "raw_knn_dt = DataFrame(columns=metrics,data=raw_accuracies)\n",
    "raw_knn_dt.insert(0,\"knn\",kns)\n",
    "raw_knn_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_knn_dt.iloc[:,1:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data from first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_accuracies = [[knnScore(embedding_umap_layer1, Y_train, embedding_umap_layer1_test, Y_test, i,metric) for metric in metrics] for i in kns]\n",
    "layer_1_knn_dt = DataFrame(columns=metrics,data=layer_1_accuracies)\n",
    "layer_1_knn_dt.insert(0,\"knn\",kns)\n",
    "layer_1_knn_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_knn_dt.iloc[:,1:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data from second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_accuracies = [[knnScore(embedding_umap_layer2, Y_train, embedding_umap_layer2_test, Y_test, i,metric) for metric in metrics] for i in kns]\n",
    "layer_2_knn_dt = DataFrame(columns=metrics,data=layer_2_accuracies)\n",
    "layer_2_knn_dt.insert(0,\"knn\",kns)\n",
    "layer_2_knn_dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_knn_dt.iloc[:,1:].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza danych\n",
    "Najlepsze wyniki:\n",
    "\n",
    "1) zredukowane dane wejściowe: 0.9548 dla knn=15 miara euclidean\n",
    "\n",
    "2) dane z warstwy 1: 0.9578 dla knn=5 miara chebyshev i euclidean\n",
    "\n",
    "3) dane z warstwy 2: 0.9697 dla knn=15 miara manhattan\n",
    "\n",
    "Najlepsza metryka: **Manhattan** i **Euclidean**\n",
    "\n",
    "Liczba sąsiadów dająca najlepsze wyniki: **15**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski\n",
    "1) Sieć neuronowa dała stosunkowo duży wzrost precyzji podczas klasyfikacji przy użyciu klasyfikatora kNN. Oznacza to, że poszczególne klasy dla danych wyjściowych ukrytych warstw jest łatwiej sklasyfikować - najbliżsi sąsiedzi częściej należą do tej samej klasy co dany element.\n",
    "\n",
    "2) Z wykonanego zadania można wywnioskować, że kolejne warstwy mają coraz mniejszy wpływ na precyzję, ale wniosek ten wymaga dalszego sprawdzenia przy użyciu większej liczby warstw.\n",
    "\n",
    "3) Liczba najbliższych sąsiadów branych pod uwagę podczas klasyfikacji ma zauważalny wpły na precyzje i posiada pewne ekstremum, dla którego trafność predykcji jest maksymalna, po czym maleje.\n",
    "\n",
    "4) Typ metryki dla klasyfikatora KNN nie miał znaczącego wpływu na ostateczną precyzję. (wpływ na poziomie 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
